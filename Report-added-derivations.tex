\documentclass[a4paper,10pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, hyperref, multirow, dot2texi}

\usepackage{tikz}
\usetikzlibrary{shapes, arrows}

% Title Page
\title{Using scores to improve language modelling movie plot summaries}
\author{Jorge S\'{a}ez G\'{o}mez\\
Roelof van der Heijden\\
Francesco Stablum
}
%\institute{Universiteit van Amsterdam}
\date{\today}

\parskip = 8pt
\setlength{\parindent}{0pt}
\DeclareMathOperator{\Dir}{Dir}
\DeclareMathOperator{\Mult}{Mult}
\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\eps}{\epsilon}

\begin{document}
\maketitle
\begin{abstract}
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec velit est, fringilla quis mollis in, dapibus nec ipsum. Donec volutpat sapien nec nibh suscipit vehicula. In hac habitasse platea dictumst. Phasellus mattis, enim sit amet tincidunt auctor, ligula ipsum fermentum libero, ut gravida mauris risus ac magna. Vestibulum a tempor mi. Donec viverra feugiat magna, eget lobortis neque volutpat eu. Nullam vehicula vitae nunc in aliquet. Ut vulputate eget eros quis mollis. Curabitur eget egestas est. Vestibulum tincidunt nisl nec justo hendrerit, in ullamcorper mauris porta. Nullam erat tortor, aliquam non purus nec, facilisis sodales risus.
\end{abstract}

\section{Introduction} 
%• Description of the problem area and the problem itself
%• What is the research question / goal?
%• Why is this an important / meaningful / interesting problem to consider?
%• The very basic idea of the approach and why this is a reasonable approach for this problem?
In recent years, several successes have been booked for applying semantic analysis on user comments of movies.
In this report we use those same techniques, but apply them to plot summaries of movies.
Using these corpus of text we try to determine whether the contents of these summaries and the score these movies are rated with on the popular online movie database IMDb \cite{IMDb} are correlated.
We do this by comparing the performance on two latent Dirichlet allocation models -- one with and another without using the scores.

This project is part of the Natural Language Processing course of the UvA from Fall 2014.

\section{Problem}
%• Explain the problem; what kind of assumptions / observations you have about the problem
In this section we describe the characteristics of the problem and take a closer look at the data set that we use.

%\subsection{Assumptions and observations}
%The core assumption of this project is that 

\subsection{Data set}
The texts that we use in ths model are summaries of movies.
These summaries have been written by users of the popular online movie database IMDb, with the intent to outline the events that occur in the movie.

This is fundamentally different from movie reviews, as the author is not supposed to convey his or her own opinion of the movie in the summary.
However, this can obviously never be fully prevented, since the author has seen the movie in question and is willing to spend time and effort to write the summary.
In this light we make the following assumptions.

\begin{enumerate}
  \item An important assumption we make is that the authors wrote the summaries voluntarily, without any compensation or external influence which might affect the writing of the author.
  \item Additionally, we assume that the summary also contains the authors personal opinion on the movie, although this does not have to be explicitly mentioned.
\end{enumerate}
Only if these assumptions hold, can we try to find a correlation between the summary and the score of the movie.

An example plot summary from IMDb can be found below. 
It is from the movie Big Fish (2003) and is written by a user who wanted to remain anonymous.
\begin{quotation}
  \emph{The story revolves around a dying father and his son, who is trying to learn more about his dad by piecing together the stories he has gathered over the years.
  The son winds up re-creating his father's elusive life in a series of legends and myths inspired by the few facts he knows.
  Through these tales, the son begins to understand his father's great feats and his great failings.}
  \flushright{by Anonymous}
\end{quotation}

\section{Approach}
%• Explain the model; if any important assumptions are made at this stage, explain why they are reasonable or necessary
%• Explain learning / inference algorithms
%• Explaining (perhaps briefly) any necessary preprocessing / postprocesing / data acquisition stages (maybe earlier, depending on the project; may also move to the experimental section)

\subsection{Model}
In this section we describe the extended topic based model we used.
It is taken from \cite{SLDA}.

The generative version of LDA is as follows:
\begin{enumerate}
  \item Draw topic proportions $\theta \mid \alpha \sim \Dir(\alpha)$.
  \item For each word:
  \begin{enumerate}
    \item Draw topic assignment $z_n \mid \theta \sim \Mult(\theta)$.
    \item Draw word $w_n \mid z_n, \beta_{1:K} \sim \Mult(\beta_{zn})$.
  \end{enumerate}
\end{enumerate}
Its graphical representation can be seen in Figure~\ref{fig:LDA}.
It makes use of the hyperparameters $\alpha$, which influences the topic distributions of a document, and of $\beta$, which influences the topic definitions of each topic.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\textwidth]{LDA.png}
  \label{fig:LDA}
  \caption{A graphical representation of traditional LDA model.}
\end{figure}

We however, use an extended version of LDA, which makes use of the given scores. 
Because of this, a third step is added to the generative process:

\begin{enumerate}
  \item Draw topic proportions $\theta \mid \alpha \sim \Dir(\alpha)$.
  \item For each word:
  \begin{enumerate}
    \item Draw topic assignment $z_n \mid \theta \sim \Mult(\theta)$.
    \item Draw word $w_n \mid z_n, \beta_{1:K} \sim \Mult(\beta_{zn})$.
  \end{enumerate}
  \item[3.] Draw response variable $y \mid z_{1:N}, \eta, \sigma^2 \sim \mathcal{N}(\eta^\top \bar{z}, \sigma^2)$.
\end{enumerate}

This version can be called supervised LDA or SLDA.
Its graphical representation can be seen in Figure~\ref{fig:SLDA}.
Note that we use a symmetric $\alpha$ and $\beta$, i.e. $\alpha=(\alpha_1, \alpha_2, \dots, \alpha_K)$, where $\forall i,j \leq K: \alpha_i = \alpha_j$ and similar for $\beta$.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\textwidth]{SLDA.png}
  \label{fig:SLDA}
  \caption{A graphical representation of our modified LDA model.}
\end{figure}

\subsubsection{Collapsed Supervised Latent Dirichlet Allocation}

We start by recalling the full likelihood expression (i.e. for all variables, both latent and visible) of the model:

\[P(\Phi, \Theta, S, Z, W \mid \alpha, \beta, \eta, \sigma) = \left[ \prod_{k = 1}^K \Dir(\phi_k \mid \beta) \right]  \]
\begin{equation}
\label{eq:prodpart}
\times \left[ \prod_{d = 1}^D \Dir(\theta_d \mid \alpha) \mathcal{N}(s_d \mid \eta^\top \bar{z}_d, \sigma) \prod_{i = 1}^{N_d} \Mult(z_{di} \mid \theta_d) \Mult(w_{di} \mid \varphi_{z_{di}}) \right]
\end{equation}
\[\times \left[ \prod_{d} \Dir(\theta_d \mid \alpha) \prod_d \Mult(\phi_d \mid \theta_d )\Mult(w_d \mid \phi_d)\right]\]
\[\times \int_{\phi_0}\int_{\phi_1}\cdots \int_{\phi_k} P(\theta, s, z, \phi, W \mid \alpha, \beta, \eta, \sigma)\]
\[=\left[ \prod_k \Dir(\phi_k \mid \beta) \right] \left[ \prod_d \Dir(\theta_d \mid \alpha) \mathcal{N}(\eta^\top \bar{z}_d, \sigma) \prod_i^{N_d} \Mult(z) \right]\]
\[\times \left[  \prod_d \prod_i^{N_d} \Mult(w_{di} \mid \phi_{z_d})\right] \rightarrow \prod_d \prod_w \prod_k \left[ \Mult(w \mid \phi_k)^{N_{dk}}\right]\]

\[P(\theta, s, z, w \mid \alpha, \beta, \eta, \sigma) = \left[ \prod_k \int_{\phi_k} \Dir(\phi_k \mid \beta) \prod_d \prod_w \left[ \Mult(w \mid \phi_k)^{N_{dk}}\right] \right]\]
\[ = \left[ \prod_d \Dir(\theta_d \mid \alpha) \mathcal{N}(s_d \mid \eta^\top \bar{z}_d, \sigma) \prod_i^N \Mult(z_{di} \mid \theta_d) \right]\]
\[\times \left[ \prod_k \frac{\Gamma(\beta) \Gamma(W\beta)}{\Gamma(N_k + W\beta)} \prod_w \frac{\Gamma(N_{kw}+\beta)}{\Gamma(\beta)}\right]\]

We will first integrate out the latent variable $\Phi$. The first thing that can be noticed is that every $\varphi_k$ is independently sampled, and thus can be integrated separately:
\begin{equation}
P(\Theta, S, Z, W \mid \alpha, \beta, \eta, \sigma) = \int_{\phi_0}\int_{\phi_1}\cdots \int_{\phi_K} P(\Phi, \Theta, S, Z, W \mid \alpha, \beta, \eta, \sigma)
\end{equation}
At this point we need to rewrite part of Equation~\ref{eq:prodpart} in order to be able to continue the derivation:
\begin{equation}
\prod_{d = 1}^D \prod_{i = 1}^{N_d} \Mult(w_{di} \mid \phi_{z_{di}}) = \prod_{d = 1}^D \prod_{w = 1}^W \prod_{k = 1}^K \Mult(w \mid \phi_k)^{N_{dk}}
\end{equation}
Where $N_d$ represents the total number of words within document $d$, and $N_{dk}$ represents the number of words within document $d$ assigned to topic $k$.
\begin{equation}
\begin{gathered}
P(\Theta, S, Z, W \mid \alpha, \beta, \eta, \sigma) = \left[ \prod_{k = 1}^K \int_{\phi_k} \Dir(\phi_k \mid \beta) \prod_{d = 1}^D \prod_{w = 1}^W \Mult(w \mid \phi_k)^{N_{dk}} \right]  \\
\times\left[ \prod_{d = 1}^D \Dir(\theta_d \mid \alpha) \mathcal{N}(s_d \mid \eta^\top \bar{z}_d, \sigma) \prod_{i = 1}^{N_d} \Mult(z_{d, i} \mid \theta_d) \right]
\end{gathered}
\end{equation}
We can now make use of the definition of the Dirichlet-Multinomial distribution in order to solve all the integrals:
\begin{equation}
\begin{gathered}
P(\Theta, S, Z, W \mid \alpha, \beta, \eta, \sigma) = \left[ \prod_{k = 1}^K \frac{ \Gamma(W\beta)}{\Gamma(N_k + W\beta)} \prod_{w = 1}^W \frac{\Gamma(N_{kw}+\beta)}{\Gamma(\beta)}\right]\\
\times\left[ \prod_{d = 1}^D \Dir(\theta_d \mid \alpha) \mathcal{N}(s_d \mid \eta^\top \bar{z}_d, \sigma) \prod_{i = 1}^{N_d} \Mult(z_{di} \mid \theta_d) \right]
\end{gathered}
\end{equation}
We can proceed in an analogous fashion in order to integrate out the latent $\Theta$ parameter:
\begin{equation}
P(S, Z, W \mid \alpha, \beta, \eta, \sigma) = \int_{\theta_0}\int_{\theta_1}\dots \int_{\theta_D} p(\Theta, S, Z, W \mid \alpha, \beta, \eta, \sigma)
\end{equation}
This gives us:
\begin{equation}
\begin{gathered}
P(S, Z, W \mid \alpha, \beta, \eta, \sigma) =  \left[ \prod_{k = 1}^K \frac{ \Gamma(W\beta)}{\Gamma(N_k + W\beta)} \prod_{w = 1}^W \frac{\Gamma(N_{kw}+\beta)}{\Gamma(\beta)}\right]  \\
\times\left[ \prod_{d = 1}^D \mathcal{N}(s_d \mid \eta^\top \bar{z}_d, \sigma) \frac{\Gamma(K\alpha)}{\Gamma(N_d + K \alpha)} \prod_{k = 1}^K \frac{\Gamma(N_{dk} + \alpha)}{\Gamma(\alpha)} \right]
\end{gathered}
\end{equation}
Note that $\Gamma(\beta)$ and $\Gamma(\alpha)$ are constant in these products, so they can be moved outside of the product.
\begin{equation}
\begin{gathered}
P(S, Z, W \mid \alpha, \beta, \eta, \sigma) = \left[ \prod_{k = 1}^K \frac{ \Gamma(W\beta)}{\Gamma(\beta)^W \Gamma(N_k + W\beta)} \prod_{w = 1}^W \Gamma(N_{kw}+\beta)\right]\\
\times\left[ \prod_{d = 1}^D \mathcal{N}(s_d \mid \eta^\top \bar{z}_d, \sigma) \frac{\Gamma(K\alpha)}{\Gamma(\alpha)^K \Gamma(N_d + K \alpha)} \prod_{k = 1}^K \Gamma(N_{dk} + \alpha) \right]
\end{gathered}
\end{equation}

\subsubsection{Collapsed Gibbs Sampler}

Using Bayes' theorem, we know that:

\begin{equation}
P(Z \mid S, W, \dots) = \frac{P(S, W \mid Z, \dots) P(Z \mid \dots)}{P(S, W \mid \dots)} \propto P(S, W, Z \mid \dots)
\end{equation}

Where we did not write all model hyperparameters for the sake of clarity.
Thus, in order to implement a Gibbs sampler for this model, we have that:
\[P(z_{di} = k \mid Z^{\backslash{i}}, S, W, \alpha, \beta, \eta, \sigma) \propto P(z_{di} = k, Z^{\backslash{i}}, S, W, \alpha, \beta, \eta, \sigma) = \]
More formulas still need to be written here... % TODO...

We can further simplify the previous expression by removing those factors that are constant across all possible values for $k$, resulting in the following final collapsed Gibbs sampler:
\[P(z_{di} = k \mid Z^{\backslash{i}}, S, W, \alpha, \beta, \eta, \sigma) \propto \left[ \prod_{k'} \frac{\prod_w \Gamma(N_{{k'}w}^{\backslash i} + \mathbb{I}(k' = k \wedge w = w_{di}) + \beta)}{\Gamma(N_{k'}^{\backslash i} + \mathbb{I}(k' = k) + W \beta)} \right] \times \]
\begin{equation}
\mathcal{N}\left(s_d\, \left|\, \eta^T \cdot \frac{N_{d{k'}}^{\backslash i} + \mathbb{I}(k' = k)}{N_d}, \sigma\right. \right) \prod_{k'} \Gamma(N_{d{k'}}^{\backslash i} + \mathbb{I}(k' = k) + \alpha)
\end{equation}

Where we used $ \frac{N_{dk}}{N_d} \equiv \bar{z_d} $. 
The Gibbs sampler can also be expressed in log-space probabilities, in order to get a more numerically-stable implementation:
\[ \log P(z_{di} = k \mid Z^{\backslash{i}}, S, W, \alpha, \beta, \eta, \sigma)  \]
\[ \propto\sum_{k',w} \left[ \log \Gamma(N_{{k'}w}^{\backslash i} + \mathbb{I}(k' = k \wedge w = w_{di}) + \beta) - \log \Gamma(N_{k'}^{\backslash i} + \mathbb{I}(k' = k) + W \beta) \right] \]
\[ \hspace{10pt}-\frac{1}{2 \sigma^2}\left(s_d - \eta^\top \frac{N_{d{k'}}^{\backslash i} + \mathbb{I}(k' = k)}{N_d}\right)^2 + \sum_{k'} \log \Gamma(N_{d{k'}}^{\backslash i} + \mathbb{I}(k' = k) + \alpha) \]

\subsubsection{Rewriting the log-gamma function}
The logarithm of the gamma function can be rewritten as follows \cite{BorosMoll}:
\begin{equation}
  \log \Gamma(z) = -\gamma z - \log z + \sum_{j=1}^\infty \left[\frac{z}{j}-\log\left(1+\frac{z}{j}\right)\right]
\end{equation}
where $\gamma$ is the Euler-Mascheroni constant. We apply this to $\sum_k \sum_w \log \Gamma(N_{kw} + \beta)$, which then becomes:
\begin{align*}
&=\sum_{k, w} -\gamma (N_{kw} + \beta) -\log (N_{kw} + \beta) + \sum_{j=1}^\infty \frac{N_{kw} +\beta}{j}-\log\left(1 + \frac{N_{kw} + \beta}{j}\right)\\
&=-\gamma (N + KW\beta) - \sum_{k,w} \log (N_{kw}+\beta) - \sum_{j=1}^\infty  \frac{N_{kw} + \beta}{j} - \log\left(\frac{N_{kw} + \beta+j}{j}\right)\\
\intertext{Note that the term \(-\gamma (N + KW\beta)\) serves as a normalisation constant for this dataset.
Since we do not need the exact probabilities but only the proportional probabilities during the algorithms execution, we can discard those terms.}
&\Rightarrow - \sum_{k,w} \log (N_{kw}+\beta) - \sum_{j=1}^\infty  \frac{N_{kw} + \beta}{j} - \log\left(\frac{N_{kw} + \beta+j}{j}\right)\\
&=-\sum_{k,w} \log (N_{kw}+\beta) - \sum_{j=1}^\infty  \frac{N_{kw} + \beta}{j} - \log(N_{kw} + \beta+j) + \log(j)\\
&=-\sum_{k,w} \log (N_{kw}+\beta) - \sum_{j=1}^\infty \left( \frac{N_{kw} + \beta}{j} +\log(j)\right) + \sum_{j=1}^\infty \log(N_{kw} + \beta + j)\\
&=\sum_{j=1}^\infty \left( \frac{N + KW\beta}{j} +KW\log(j)\right) - \sum_{k,w} \log (N_{kw}+\beta) + \sum_{j=1}^\infty \log(N_{kw} + \beta + j)\\
&=\sum_{j=1}^\infty \left( \frac{N + KW\beta}{j} +KW\log(j)\right) - \sum_{k,w} \sum_{j=0}^\infty \log(N_{kw} + \beta + j)
\end{align*}

Again, the term \(\sum_{j=1}^\infty  \frac{N + KW\beta}{j} +KW\log(j)\) is a constant for this dataset, so we can discard it.
This results in the following proportionality:
\begin{align}
  \sum_{k,w} \log \Gamma(N_{kw} + \beta) &\propto  -\sum_{k,w}\sum_{j=0}^\infty \log (N_{kw} + \beta + j)
\intertext{Using similar steps, we can also simplify}
  \sum_k \log\Gamma (N_k + W\beta) &\propto -\sum_k \sum_{j=0}^\infty \log(N_k + W\beta + j)\\
  \sum_{k,d} \log \Gamma(N_{dk} + \alpha) &\propto -\sum_{k,d} \sum_{j=0}^\infty \log(N_{dk}+\alpha +j)\\
  \sum_d \log \Gamma (N_d + K\alpha) &\propto -\sum_d \sum_{j=0}^\infty \log(N_d + K\alpha + j)
\end{align}

Putting these together gives us the following proportionality:

\[ \log P(z_{di} = k \mid Z^{\backslash{i}}, S, W, \alpha, \beta, \eta, \sigma)  \]
\[\propto \sum_{j=0}^\infty \sum_{k'}\log(N_{k'}^{\backslash i} + \mathbb{I}(k' = k) + W \beta + j) -\log(N_{d{k'}}^{\backslash i} + \mathbb{I}(k' = k) + \alpha +j)\]
\[-\sum_{j=0}^\infty \sum_{k',w} \log(N_{{k'}w}^{\backslash i} + \mathbb{I}(k' = k \wedge w = w_{di}) + \beta + j) -\frac{1}{2 \sigma^2}\left(s_d - \eta^\top \frac{N_{d{k'}}^{\backslash i} + \mathbb{I}(k' = k)}{N_d}\right)^2\]

In practice, however, we can not calculate the infinite terms of the above sums, and thus we would stop the approximation at some arbitrary iteration $J$:

\[ \log P(z_{di} = k \mid Z^{\backslash{i}}, S, W, \alpha, \beta, \eta, \sigma) \]
\[ \propto -\frac{1}{2 \sigma^2}\left(s_d - \eta^\top \frac{N_{d{k'}}^{\backslash i} + \mathbb{I}(k' = k)}{N_d}\right)^2 + \sum_{k'} \sum_{j=0}^J \log(N_{k'}^{\backslash i} + \mathbb{I}(k' = k) + W \beta + j) \]
\[ -\log(N_{d{k'}}^{\backslash i} + \mathbb{I}(k' = k) + \alpha +j) + \sum_{w} \log(N_{{k'}w}^{\backslash i} + \mathbb{I}(k' = k \wedge w = w_{di}) + \beta + j) \]

\subsubsection{Estimating response parameters}
The maximum a posteriori (MAP) estimate for the hyperparameter \(\eta\)  can be inferred from the gradient of the complete model likelihood:
\[ \nabla_{\eta_k} \log P(S, Z, W \mid \alpha, \beta, \eta, \sigma) = \sum_d \frac{1}{\sigma^2} \frac{N_{dk}}{N_d} \left( s_d - \eta^T \frac{N_{d\cdot}}{N_d}\right)  = \]
\[ = \sum_d \frac{s_d \frac{N_{dk}}{N_d} }{\sigma^2} - \sum_d \frac{ \frac{N_{dk}}{N_d} \left( \eta^T \frac{N_{d\cdot}}{N_d} \right) }{\sigma^2}\]
For the MAP estimate, the gradient should be zero.
This allows us to rewrite the above formula into:
\begin{align*}
\sum_d s_d \frac{N_{dk}}{N_d} &= \sum_d \frac{N_{dk}}{N_d} \left( \sum_{k'} \eta_{k'} \frac{N_{dk'}}{N_d} \right)\\
\sum_d s_d \frac{N_{dk}}{N_d} &= \sum_d \frac{N_{dk}}{N_d} \left( \eta_k \frac{N_{dk}}{N_d} + \sum_{k' \ne k} \eta_{k'} \frac{N_{dk'}}{N_d} \right) \\
\sum_d s_d \frac{N_{dk}}{N_d} &= \eta_k \sum_d \left( \frac{N_{dk}}{N_d}  \right)^2 + \sum_d \left( \frac{N_{dk}}{N_d} \sum_{k' \ne k} \eta_{k'} \frac{N_{dk'}}{N_d} \right) \\
\sum_d \left( s_d \frac{N_{dk}}{N_d} - \frac{N_{dk}}{N_d} \sum_{k' \ne k} \eta_{k'} \frac{N_{dk'}}{N_d} \right) &= \eta_k \sum_d \left( \frac{N_{dk}}{N_d}  \right)^2 \\
\sum_d \frac{N_{dk}}{N_d} \left( s_d - \sum_{k' \ne k} \eta_{k'} \frac{N_{dk'}}{N_d} \right) &= \eta_k \sum_d \left( \frac{N_{dk}}{N_d}  \right)^2 \\
\eta_k &= \frac{\sum_d \frac{N_{dk}}{N_d} \left( s_d - \sum_{k' \ne k} \eta_{k'} \frac{N_{dk'}}{N_d} \right)}{\sum_d \left( \frac{N_{dk}}{N_d}  \right)^2} 
\end{align*}
From our experiments, we noticed that trying to apply the previous formula as an update rule for \(\eta\) does not converge.
Instead, the following update can be used:
\begin{equation}
\eta_k^{new} \leftarrow (1 - \gamma) \eta_k^{old} + \gamma \frac{\sum_d \frac{N_{dk}}{N_d} \left( s_d - \sum_{k' \ne k} \eta_{k'} \frac{N_{dk'}}{N_d} \right)}{\sum_d \left( \frac{N_{dk}}{N_d}  \right)^2 + \epsilon},
\end{equation}
where we used $1 \gg \gamma > 0$ in order for the previous series to converge and $1 \gg \epsilon > 0$ as a smoothing constant.

\section{Experiments}
%• Any details about experiments (dataset sizes, parameter selection, etc)
%• Results
%• Analysis (discussion of results / visualization / findings / etc)

For our experiments we chose to use two different performance measures: perplexity and inverse accuracy.

\subsection{Perplexity}

The perplexity measure is commonly used within LDA literature, and we include it for reference purposes. 
It is defined as follows:
\begin{equation}
\text{Perplexity} \equiv \exp \left\{ -\frac{1}{N} \sum_{i = 1}^N \log P(x_i)  \right\},
\end{equation}
where \(P(x_i)\) represents the probability the model gives for the $i$-th item within the test set. The lower the perplexity, the less ``surprised'' the model is of seeing its input, which indicates a better model.
This measure has a shortcoming, though. 
It is enough for the model to give a probability of zero to a word to drive this performance measure to infinity. 
This does not mean, however, that the model is infinitely bad, since the rest of the items might still have large probabilities.
To overcome this problem, we also measure the performance in terms of the average inverse accuracy of the model.

\subsection{Inverse accuracy}

The average inverse accuracy of the model is defined as follows:
\begin{equation}
\text{Accuracy}^{-1} \equiv \frac{1}{\frac{1}{N}\sum_{i = 1}^N P(x_i)}
\end{equation}
This measure should be interpreted as the number of words the model incorrectly predicts for each correctly predicted word. 
The lower this magnitude, the better the model.

\subsection{Experiments}

We created a first experiment in order to check the validity of our approximation for the Gibbs sampler of our model. 
We selected different values for $J$ and compared the predictive performance for each case. 
We ran three different MCMC chains and then averaged the results. 
Within each execution, 100 movies were randomly selected as the training set, and 20 as the testing set. 
We used 5 samples, taken every 4 steps of the Gibbs sampler, with the first 20 iterations discarded as the burn-in period. 
Our findings are summarized in Table~\ref{table:J}:

\begin{table}[ht!]
\begin{center}
\begin{tabular}{r|cccccc}
J               & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
Perplexity      & 4100 & 4125 & 4082 & 4251 & 3754 & 3881 \\
Accuracy$^{-1}$ & 858 & 871 & 823 & 863 & 877 & 815 \\
\end{tabular}
\end{center}
\caption{Experiment results for varying values of $J$ with 100/20 movies as training/test set.}
\label{table:J}
\end{table}

We can observe that the predictive performance tends to improve with larger values of $J$. 
This is to be expected, since the higher $J$ is, the better the approximation of the log-gamma function becomes.
This improvement is not, however, very consistent even when averaging across 3 chains, and thus we theorize that the extra computational time spent on making $J$ higher is better used if $J$ is kept as low as possible and more movies and/or iterations are used instead.
We then conclude this approximation is interesting, at least from a computational perspective.

We devised a second experiment in order to find out good values for the number of topics to use in our models.
Again we did an equivalent set up to the one for the previous experiment, but this time changing the number of topics and whether or not the Gibbs sampler uses the movie score information.

\begin{figure}[ht!]
	Perplexity measure:
	
	\begin{center}
	\begin{tabular}{cc|ccc}
		LDA topics      & & 25 & 50 & 100 \\ \hline
		\multirow{2}{*}{Using scores?} & No  & 4128 & 5333 & 6954 \\
		                               & Yes & \textbf{4005} & 5503 & 7082 \\
	\end{tabular}
	\end{center}
	
	Inverse accuracy measure:
	
	\begin{center}
	\begin{tabular}{cc|ccc}
		LDA topics      & & 25 & 50 & 100 \\ \hline
		\multirow{2}{*}{Using scores?} & No  & 845 & 1318 & 2049 \\
		                               & Yes & \textbf{805} & 1372 & 2067 \\
	\end{tabular}
	\end{center}
\caption{Experiment results for varying values of $K$, with $J=1$ and 100/20 movies as training/test set.}
\label{figure:K}
\end{figure}

We can see how all models start overfitting with, at least, more than 25 topics.
This is most likely due to the fact that we only used 100 movie plot summaries for training, which is a rather small dataset. 
We also observe that, for 25 topics, not only the best predictive performance is achieved, but this is further improved by using the movie scores.
This improvement is very small, but we could systematically observe it in all the tests we run for this work. 

\section{Discussion}
%• Refer to the research questions you defined in your introduction.
%• Any related work you are aware of?
%• Challenges you observed?
%• “Future work” (you do not need to do this work really J, but what would you change in the model / what experiments you would run / etc, if you would have a chance to do this? What other people should look into?
%• Any thoughts / observation / wider implications

\begin{thebibliography}{9}
\bibitem{SLDA}
  David M. Blei, Jon D. McAuliffe;
  \emph{Supervised Topic Models};
  Neural Information Processing Systems 21 (2007)
  
\bibitem{IMDb}
  IMDb.com, Inc.; 
  \emph{IMDb, the world's most popular and authorative source for movie, TV and celebrity content};
  \url{www.imdb.com}

\bibitem{BorosMoll}
  Geogre Boros, Victor H. Moll,
  \emph{Irresistible Integrals: symbolics, analysis and experiments in the evaluation of integrals};
  Cambridge University Press (2004)

\end{thebibliography}


\end{document}          
